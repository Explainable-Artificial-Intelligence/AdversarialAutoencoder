# Adversarial Autoencoder


## MNIST: 

### Unsupervised Autoencoder
<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mnist/mnist_unsupervised_z_dim_2_reconstruction_grid.png?raw=True" />
  <figcaption>Reconstruction of the MNIST data set after 50 and 1000 epochs. Every other column, starting from the first, shows the original images. The column right next to it shows the respective reconstruction. Failed reconstructions are marked with a red ellipse.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mnist/z_dim_2_50_real_dist_and_encoder_dist.png?raw=True" />
  <figcaption>Prior distribution used to train the unsupervised AAE on the MNIST dataset and the data distribution on the latent space after 50 epochs. The AAE has effectively clustered the digits based on their label without any label information.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mnist/mnist_unsupervised_z_dim_2_50_latent_space_class_distribution.png?raw=True" />
  <figcaption>Data distribution of the unsupervised AAE on the latent space (left) and the images generated by sampling the latent space (right) after 50 epochs of training. </figcaption>
</figure>


### Supervised Autoencoder
<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mnist/mnist_supervised_500_latent_space_class_distribution.png?raw=True" />
  <figcaption>Data distribution of the supervised AAE on the latent space (left) and the images generated by sampling the latent space (right) after 500 epochs of training. The latent space represents the style of the digits and does not store any class label information. Each column on the right has been generated by passing the point on the latent space marked with the symbol of the column through the decoder with varying class labels (0 to 9).</figcaption>
</figure>

### Semi-supervised Autoencoder
<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mnist/mnist_semi_supervised_z_dim_2_100_real_dist_and_encoder_dist.png?raw=True" />
  <figcaption>Prior distribution used to train the semi-supervised AAE on the MNIST dataset and its data distribution on the latent space after 100 epochs. The prior distribution is a mixture of ten gaussians aligned in a star shape where each gaussian refers to a certain class. After 100 epochs the AAE has learned to map certain classes to certain locations (gaussians) on the latent space. By sampling the respective gaussians one can generate images for the class the gaussian refers to.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mnist/mnist_semi_supervised_100_gen_images.png?raw=True" />
  <figcaption>Images generated by the semi-supervised AAE after being trained on MNIST. Each column was generated by sampling randomly from the respective gaussian distribution on the latent space.</figcaption>
</figure>

## SVHN:

### Unsupervised Autoencoder
<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/svhn/svhn_unsupervised_200_reconstruction_grid.png?raw=True" />
  <figcaption>Reconstruction of the SVHN data set after 200 epochs using the unsupervised AAE. Every other column, starting from the first, shows the original images. The column right next to it shows the respective reconstruction. Failed reconstructions are marked with a red ellipse.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/svhn/svhn_unsupervised_200_latent_space_class_distribution.png?raw=True" />
  <figcaption>Data distribution of the unsupervised AAE on the latent space (left) and the images generated by sampling the latent space (right) after 200 epochs of training.</figcaption>
</figure>

### Supervised Autoencoder
<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/svhn/svhn_supervised_250_latent_space_class_distribution.png?raw=True" />
  <figcaption>Data distribution of the supervised AAE on the latent space (left) and the images generated by sampling the latent space (right) after 250 epochs of training. The latent space represents the style of the digits and does not store any class label information. Each column on the right has been generated by passing the point on the latent space marked with the symbol of the column through the decoder with varying class labels (0 to 9).</figcaption>
</figure>

### Semi-supervised Autoencoder
<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/svhn/schn_semi_supervised_250_latent_space_class_distribution.png?raw=True" />
  <figcaption>Data distribution of the semi-supervised AAE on the latent space (left) and the images generated by sampling the latent space (right) after 250 epochs of training.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/svhn/svhn_semi_supervised_500_gen_images.png?raw=True" />
  <figcaption>Images generated by the semi-supervised AAE after being trained on SVHN for 250 epochs. Each column was generated by sampling randomly from the respective gaussian distribution on the latent space.</figcaption>
</figure>


## Mass Spectrometry data:

### Unsupervised Autoencoder

#### Undercomplete Autoencoder

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mass_spec/spectra_generation/undercomplete_autoencoder/1000_mass_specs_spectra_small.png?raw=True" />
  <figcaption>Spectra reconstruction of four random spetra using the undercomplete AAE. The m/z loss is 38.34, wheras the intensity loss is 80.82 Da per peak.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mass_spec/spectra_generation/undercomplete_autoencoder/generated_spectra.png?raw=Truee" />
  <figcaption>Generated spectra using the undercomplete AAE.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mass_spec/spectra_generation/undercomplete_autoencoder/mz_intensity_distributions.png?raw=True" />
  <figcaption>M/Z and intensity distributions of the original, reconstructed and generated spectra of the undercomplete AAE.</figcaption>
</figure>

#### Overcomplete Autoencoder

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mass_spec/spectra_generation/overcomplete_autoencoder/1000_mass_specs_spectra_small.png?raw=True" />
  <figcaption>Spectra reconstruction of four random spetra using the overcomplete AAE. The m/z loss is 10.9, wheras the intensity loss is 6.3 Da per peak.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mass_spec/spectra_generation/overcomplete_autoencoder/generated_spectra.png?raw=True" />
  <figcaption>Generated spectra using the overcomplete AAE.</figcaption>
</figure>

<figure align="center">
  <img src="https://github.com/Explainable-Artificial-Intelligence/AdversarialAutoencoder/blob/master/doc/results/mass_spec/spectra_generation/overcomplete_autoencoder/mz_intensity_distributions_large.png?raw=True" />
  <figcaption>M/Z and intensity distributions of the original, reconstructed and generated spectra of the overcomplete AAE.</figcaption>
</figure>

